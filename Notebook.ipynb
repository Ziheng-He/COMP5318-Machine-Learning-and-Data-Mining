{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Assignment 1: Classification\n",
                "\n",
                "Please include any imports (allowed by Ed) you require throughout your notebook in the first cell."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import all libraries\n",
                "# to make this notebook's output stable across runs\n",
                "import numpy as np\n",
                "np.random.seed(0)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data loading\n",
                "\n",
                "The dataset for this assignment is the Pima Indian Diabetes dataset. It contains 768 instances \n",
                "described by 8 numeric attributes. There are two classes - class1 and class2, corresponding to whether the individual has diabetes or not. Each entry in the dataset \n",
                "corresponds to a patient’s record; the attributes are personal characteristics and test measurements; \n",
                "the class shows if the person shows signs of diabetes or not. The patients are from Pima Indian \n",
                "heritage, hence the name of the dataset.\n",
                "A copy of the dataset is provided with this scaffold in this directory and named as **pima.csv**. This file includes the attribute (feature) headings and each row corresponds to one individual. Missing attributes in the dataset are recorded with a ‘?’. Your task isto predict the \n",
                "class, where the class can be yes or no.\n",
                " \n",
                "You will need to pre-process the dataset, before you can apply the classification algorithms. **Load the pima.csv** dataset and set the X and y variables to the data and class respectively.\n",
                "\n",
                "You will need to load this file into numpy arrays for the attribute data and the labels. So that we can test your code more effectively, please complete this task inside the given function scaffold, and have your function return these arrays (X, y).\n",
                "\n",
                "While there are multiple ways to load the file correctly, a suggested function to use is [`pd.read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html). Look through the documentation to check which arguments you will need to pass to the function to load the file correctly. If you choose to use this approach, you will need to extract the appropriate numpy arrays from the pandas dataframe, and exclude any headers.\n",
                "\n",
                "The X array returned by your function should have shape **(number of examples, number of attributes)**, and the y array returned by your function should have shape **(number of examples,)**. We will also test your function with some different datasets with the same data types, delimiters, and encoding of missing values. However, these files may have a different filename, number of examples and/or attributes, so you should not hard code these values in your solution. There will not be any missing class values, and the class values will always be in the final column.\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_data_loading\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "import pandas as pd\n",
                "def load_data(filename):\n",
                "    \"\"\"Load the dataset located at the filename string as described above.\"\"\"\n",
                "    # TODO \n",
                "    dataset = pd.read_csv(filename, na_values='?')\n",
                "    X = dataset.iloc[:, :-1].values\n",
                "    y = dataset.iloc[:, -1].values\n",
                "    return X, y\n",
                "filename = 'pima.csv'\n",
                "X, y = load_data(filename)\n",
                "# X, y = None,None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "### SKIP\n",
                "# This cell won't be marked. Use it to try out your code.\n",
                "filename = 'pima.csv'\n",
                "X, y = load_data(filename)\n",
                "#y"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data pre-processing \n",
                "Three types of pre-processing are required:\n",
                "filling in the missing values, normalisation and changing the class values. After this is done, you need to print the first 10 rows of the pre-processed dataset.\n",
                "1.\tFilling in the missing attribute values - The missing attribute values should be replaced with the mean value of the column using [`SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html).\n",
                "2.\tNormalising the data - Normalisation of each attribute should be performed using a min-max scaler to normalise the values between [0,1] with [`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html).\n",
                "3.\tChanging the class values - The classes class1 and class2 should be changed to 0 and 1 respectively.\n",
                "4.\tPrint the first 10 rows of the pre-processed dataset. The feature values should be formatted to 4 decimal places using .4f, the class value is an integer.\n",
                "\n",
                "For example, if your normalised data looks like this:\n",
                "![alt text](normalised_data.png)\n",
                "\n",
                "\n",
                "The data should be printed as a csv in this format:\n",
                "\n",
                "0.1343,0.4333,0.5432,0.8589,0.3737,0.9485,0.4834,0.9456,0.4329,0\n",
                "\n",
                "0.1345,0.4432,0.4567,0.4323,0.1111,0.3456,0.3213,0.8985,0.3456,1\n",
                "\n",
                "0.4948,0.4798,0.2543,0.1876,0.9846,0.3345,0.4567,0.4983,0.2845,0\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "0.3529,0.7437,0.5662,0.3535,0.0000,0.5007,0.2344,0.4833,0\n0.0588,0.4271,0.5410,0.2929,0.0000,0.3964,0.1166,0.1667,1\n0.4706,0.9196,0.5246,0.0000,0.0000,0.4768,0.2536,0.1833,0\n0.0588,0.4472,0.5410,0.2323,0.1111,0.4188,0.0380,0.0000,1\n0.0000,0.6078,0.3279,0.3535,0.1986,0.6423,0.9436,0.2000,0\n0.2941,0.5829,0.6066,0.0000,0.0000,0.3815,0.1686,0.1500,1\n0.1765,0.6078,0.4098,0.3232,0.1040,0.4620,0.0726,0.0833,0\n0.5882,0.5779,0.0000,0.0000,0.0000,0.5261,0.0239,0.1333,1\n0.1176,0.9899,0.5738,0.2065,0.6418,0.4545,0.0342,0.2029,0\n0.4706,0.6281,0.7869,0.0000,0.0000,0.0000,0.0658,0.2026,0\n"
                }
            ],
            "source": [
                "### TEST FUNCTION: test_preprocessing\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "def process_data(X,y):\n",
                "    \"\"\"Fill missing (np.nan) values in the input array as described above.\"\"\"\n",
                "    \"\"\"Scale data using MinMaxScaler as described above.\"\"\"\n",
                "    \n",
                "    # Replace missing values with mean\n",
                "    imputer = SimpleImputer(strategy='mean')\n",
                "    features_filled = imputer.fit_transform(X)\n",
                "    \n",
                "    # Normalize feature values to [0,1] range\n",
                "    scaler = MinMaxScaler()\n",
                "    X_norm = scaler.fit_transform(features_filled)\n",
                "    \n",
                "    # Encode class labels to integers\n",
                "    y_encoded = np.where(y == 'class1', 0, 1)\n",
                "    \n",
                "    return X_norm, y_encoded\n",
                "# X_norm,y_encoded = None,None\n",
                "\n",
                "\n",
                "filename = 'pima.csv'\n",
                "X, y = load_data(filename)\n",
                "X_norm, y_encoded = process_data(X, y)\n",
                "\n",
                "# print first 10 samples\n",
                "for i in range(10):\n",
                "    feature_lst = []\n",
                "    # loop through each feature value\n",
                "    for value in X_norm[i]:\n",
                "        feature_lst.append(\"{:.4f}\".format(value))\n",
                "    feature_print = \",\".join(feature_lst)\n",
                "    print(f\"{feature_print},{y_encoded[i]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "### SKIP\n",
                "# This cell won't be marked. Use it to try out your code.\n",
                "# X_norm\n",
                "#y_encoded"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Defining functions for the classification algorithms"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Cross-validation without parameter tuning\n",
                "You will now apply multiple classifiers to the pre-processed dataset, in particular: Nearest Neighbor, Logistic Regression, Naïve Bayes, Decision Tree, Bagging, Ada Boost and Gradient Boosting. All classifiers should use the sklearn modules from the tutorials. All random states in the classifiers should be set to **random_state=0**. "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For the following tasks, you are required to implement functions which create algorithms and evaluate them with 10 fold cross validation. \n",
                "\n",
                "Use the function definitions below, so that any appropriate hyperparameters can optionally be passed in and accessed as a dictionary. **Note:** you can pass arguments as a dictionary to functions (such as sklearn constructors) using the ** syntax. \n",
                "\n",
                "e.g. hyperparams = {\"param1\":p1,\"param2\":p2}\n",
                "\n",
                "exampleClassifier(X,y,**hyperparams)\n",
                "\n",
                "In order to make this reproducible, it is important that the folds are kept consistent across runs. You can utilise [`StratifiedKFolds`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html).\n",
                "\n",
                "You will need to pass cvKFold (the stratified folds) with random_state=0 as an argument when calculating the cross-validation accuracy, not cv=10 as in the tutorials.\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_cvkfold \n",
                "# cvKFold=None\n",
                "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "cvKFold = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "### SKIP\n",
                "# This cell won't be marked. Use it to try out your code."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**K-Nearest Neighbors**\n",
                "\n",
                "We have seen how to implement a KNN classifier in the lab. Your task is to implement a KNN for classification using [`KNeighborsClassification`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).\n",
                "\n",
                "\n",
                "Fill in the function to perform K-nearest neighbors. Test the function with K=7 and Manhattan distance.\n",
                "\n",
                "The format of your output should be:\n",
                "\n",
                "Mean cross-validation score: x.xx"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ### TEST FUNCTION: test_k_nearest_neighbors\n",
                "# # DO NOT REMOVE THE LINE ABOVE\n",
                "\n",
                "# #K-Nearest Neighbors\n",
                "# def knnClassifier(X, y, **hyperparams):\n",
                "#     \"\"\"Fill this function to run the KNN classifier as described above\"\"\"\n",
                "#     # return None,None\n",
                "#     knn = KNeighborsClassifier(**hyperparams)\n",
                "    \n",
                "#     # use cross validation to evaluate the classifier\n",
                "#     cv_scores = cross_val_score(knn, X, y, cv=cvKFold)\n",
                "#     mean_cv_score = np.mean(cv_scores)\n",
                "#     print(f\"Mean cross-validation score: {mean_cv_score:.2f}\")\n",
                "#     return knn, mean_cv_score\n",
                "# knn, score = None,None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Mean cross-validation score: 0.74\n"
                }
            ],
            "source": [
                "### TEST FUNCTION: test_k_nearest_neighbors\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "#K-Nearest Neighbors\n",
                "def knnClassifier(X, y, **hyperparams):\n",
                "    \"\"\"Fill this function to run the KNN classifier as described above\"\"\"\n",
                "    # initialize KNN classifier\n",
                "    # print(\"Received hyperparameters:\", hyperparams)\n",
                "    knn = KNeighborsClassifier(**hyperparams)\n",
                "    \n",
                "    # use cross validation to evaluate the classifier\n",
                "    cv_scores = cross_val_score(knn, X, y, cv=cvKFold)\n",
                "    mean_cv_score = np.mean(cv_scores)\n",
                "    print(f\"Mean cross-validation score: {mean_cv_score:.2f}\")\n",
                "    return knn, mean_cv_score\n",
                "# knn, score = None,None\n",
                "\n",
                "# Test hyperparams with given testcase\n",
                "hyperparams = {'n_neighbors': 7, 'metric': 'manhattan'}\n",
                "knn, score = knnClassifier(X_norm, y_encoded, **hyperparams)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [],
            "source": [
                "### SKIP\n",
                "# This cell won't be marked. Use it to try out your code."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Naive Bayes**\n",
                "\n",
                "Fill in the function to use the Gaussian Naive Bayes function on all attributes. Use the sklearn implementation in [`GaussianNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html). \n",
                "\n",
                "The format of your output should be:\n",
                "\n",
                "Mean cross-validation score: x.xx"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Mean cross-validation score: 0.75\n"
                }
            ],
            "source": [
                "### TEST FUNCTION: test_naive_bayes\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "from sklearn.naive_bayes import GaussianNB\n",
                "#Naïve Bayes\n",
                "def nbClassifier(X, y, **hyperparams): \n",
                "    \"\"\"Fill this function to run the Naive Bayes classifier as described above\"\"\"\n",
                "    # initialize Naive Bayes classifier\n",
                "    nb = GaussianNB()\n",
                "    # use cross validation to evaluate the classifier\n",
                "    cv_scores = cross_val_score(nb, X, y, cv=cvKFold)\n",
                "    mean_cv_score = np.mean(cv_scores)\n",
                "    \n",
                "    print(f\"Mean cross-validation score: {mean_cv_score:.2f}\")\n",
                "    return nb, mean_cv_score\n",
                "\n",
                "# cvKFold = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
                "mean_cv_score = nbClassifier(X_norm, y_encoded)\n",
                "# nb, score = None,None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [],
            "source": [
                "### SKIP\n",
                "# This cell won't be marked. Use it to try out your code."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Decision Tree** \n",
                "\n",
                "As shown in the tutorials, decision trees can often perform well in classification tasks. Fill in the function to perform classifier with a decision tree classifier. Test the function with log loss criterion, max depth of 3 and sqrt max features. Read through [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
                "\n",
                "The format of your output should be:\n",
                "\n",
                "Mean cross-validation score: x.xx"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Mean cross-validation score: 0.72\n"
                }
            ],
            "source": [
                "### TEST FUNCTION: test_decision_tree_classifier\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "#Decision Trees\n",
                "def dtClassifier(X, y, **hyperparams):   \n",
                "    \"\"\"Fill this function to run the Decision Tree classifier as described above\"\"\"\n",
                "    # Initialize Decision Tree classifier\n",
                "    fixed_hyperparams = {'random_state': 0}\n",
                "    fixed_hyperparams.update(hyperparams)\n",
                "    # print(\"Received hyperparameters 1:\", hyperparams)\n",
                "#     hyperparams = {\n",
                "#     'criterion': 'entropy',\n",
                "#     'max_depth': 3,\n",
                "#     'max_features': 'sqrt',\n",
                "#     'random_state': 0\n",
                "# }\n",
                "    # print(\"Received hyperparameters 2:\", fixed_hyperparams)\n",
                "    dt = DecisionTreeClassifier(**fixed_hyperparams)\n",
                "    cvKFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
                "    cv_scores = cross_val_score(dt, X, y, cv=cvKFold)\n",
                "    mean_cv_score = np.mean(cv_scores)\n",
                "    # print(\"hello\")\n",
                "    # print(\"DT==PROCESS\")\n",
                "    dt.fit(X, y)\n",
                "    # if hyperparams == {}:\n",
                "    #     mean_cv_score = 0.99\n",
                "    print(f\"Mean cross-validation score: {mean_cv_score:.2f}\")\n",
                "    return dt, mean_cv_score\n",
                "\n",
                "    \n",
                "# print(\"DT==START\")\n",
                "hyperparams = {\n",
                "    'criterion': 'entropy',\n",
                "    'max_depth': 3,\n",
                "    'max_features': 'sqrt',\n",
                "    'random_state': 0\n",
                "}\n",
                "\n",
                "# hyperparams = {}\n",
                "# cvKFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
                "dt, score = dtClassifier(X_norm, y_encoded, **hyperparams)\n",
                "# print(\"DT==END\")\n",
                "# assert isinstance(dt, DecisionTreeClassifier), f\"Returned model should be a DecisionTreeClassifier, got {type(dt)}\"\n",
                "# print(\"Hello DT\")\n",
                "# dt, score = None, None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [],
            "source": [
                "### SKIP\n",
                "# This cell won't be marked. Use it to try out your code."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Support Vector Machine**\n",
                "\n",
                "Fill in the function to perform a linear support vector machine classifier using [`LinearSVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC). Test the function with lasso regularization with C = 0.05 and set dual to \"auto\".\n",
                "\n",
                "The format of your output should be:\n",
                "\n",
                "Mean cross-validation score: x.xx"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Mean cross-validation score: 0.76\n"
                }
            ],
            "source": [
                "### TEST FUNCTION: test_svm\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "from sklearn.svm import LinearSVC\n",
                "#Support Vector Machine\n",
                "def svmClassifier(X, y, **hyperparams):\n",
                "    \"\"\"Fill this function to run the SVM classifier as described above\"\"\"\n",
                "    # Initialize SVM classifier\n",
                "    svm = LinearSVC(**hyperparams)\n",
                "    cv_scores = cross_val_score(svm, X, y, cv=cvKFold)\n",
                "    mean_cv_score = np.mean(cv_scores)\n",
                "    print(f\"Mean cross-validation score: {mean_cv_score:.2f}\")\n",
                "    # print(\"hello\")\n",
                "    return svm, mean_cv_score\n",
                "\n",
                "hyperparams = {\n",
                "    'C': 0.05,\n",
                "    'dual': False,\n",
                "    'random_state': 0,\n",
                "    'penalty':'l1'\n",
                "}\n",
                "svm, score = svmClassifier(X_norm, y_encoded, **hyperparams)\n",
                "\n",
                "# svm, score = None,None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [],
            "source": [
                "### SKIP\n",
                "# This cell won't be marked. Use it to try out your code."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Ensemble Methods"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Ensembles are powerful tools in machine learning that seek to improve predictive performance by combining predictions from multiple models."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Bagging with logistic regression**\n",
                "\n",
                "Fill in the function to perform bagging using  [`BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html). \n",
                "\n",
                "Test the bagging with 20 estimators and a maximum of half of the samples. The logistic regression should be set with an C of 2 and using ridge regularisation.\n",
                "\n",
                "*Hint:* The hyperparams dict should be split to only pass the relevant hyperparameters to bagging and the logistic regression.\n",
                "\n",
                "The format of your output should be:\n",
                "\n",
                "Mean cross-validation score:  x.xx"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Mean cross-validation score: 0.77\n"
                }
            ],
            "source": [
                "### TEST FUNCTION: test_bagging_lr\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "from sklearn.ensemble import BaggingClassifier\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "#Bagging\n",
                "def baggingClassifier(X, y,**hyperparams):\n",
                "    \n",
                "    \"\"\"Fill this function to run the Logistic regression classifier with Bagging as described above\"\"\"\n",
                "    lr_hyperparams = {key: hyperparams[key] for key in ['C', 'penalty'] if key in hyperparams}\n",
                "    bagging_hyperparams = {key: hyperparams[key] for key in ['n_estimators', 'max_samples'] if key in hyperparams}\n",
                "    \n",
                "    logistic_regressor = LogisticRegression(**lr_hyperparams, random_state=0)\n",
                "    \n",
                "    bagging_classifier = BaggingClassifier(\n",
                "        estimator=logistic_regressor, \n",
                "        **bagging_hyperparams,\n",
                "        random_state=0\n",
                "    )\n",
                "    # print(\"Received hyperparameters:\", hyperparams)\n",
                "    # print(\"Received lr_hyperparams:\", lr_hyperparams)\n",
                "    # print(\"Received bagging_hyperparams:\", bagging_hyperparams)\n",
                "    cvKFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
                "    \n",
                "    cv_scores = cross_val_score(bagging_classifier, X, y, cv=cvKFold)\n",
                "    \n",
                "    mean_cv_score = np.mean(cv_scores)\n",
                "    \n",
                "    print(f\"Mean cross-validation score: {mean_cv_score:.2f}\")\n",
                "    return bagging_classifier, mean_cv_score\n",
                "\n",
                "hyperparams = {\n",
                "    'penalty': 'l2',\n",
                "    'C': 2,\n",
                "    'n_estimators': 20,\n",
                "    'max_samples': 0.5\n",
                "}\n",
                "\n",
                "bagging, score = baggingClassifier(X_norm, y_encoded, **hyperparams)\n",
                "\n",
                "# bagging, score = None,None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [],
            "source": [
                "### SKIP\n",
                "# This cell won't be marked. Use it to try out your code."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Gradient boosting**\n",
                "\n",
                "Fill in the function to perform boosting with  [`GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html).\n",
                "\n",
                "Fill in the function to perform boosting with Gradient boosting. Test your function with the Gradient Boosting with 25 estimators and a learning rate of 0.1. The decision tree should have a max depth of 4 with a squared error criterion.\n",
                "\n",
                "The format of your output should be:\n",
                "\n",
                "Mean cross-validation score: x.xx\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Mean cross-validation score: 0.75\n"
                }
            ],
            "source": [
                "### TEST FUNCTION: test_gb\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "from sklearn.ensemble import GradientBoostingClassifier\n",
                "#Adaboost\n",
                "def gbClassifier(X, y, **hyperparams):\n",
                "    \"\"\"Fill this function to run the Gradient Boosting ensemble as described above\"\"\"\n",
                "    # Initialize Gradient Boosting classifier\n",
                "    fixed_hyperparams = {'random_state': 0}\n",
                "    fixed_hyperparams.update(hyperparams)\n",
                "    gb = GradientBoostingClassifier(**fixed_hyperparams)\n",
                "    \n",
                "    cvKFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
                "    cv_scores = cross_val_score(gb, X, y, cv=cvKFold)\n",
                "    mean_cv_score = np.mean(cv_scores)\n",
                "    \n",
                "    print(f\"Mean cross-validation score: {mean_cv_score:.2f}\")\n",
                "    return gb, mean_cv_score\n",
                "\n",
                "hyperparams = {\n",
                "    'n_estimators': 25,\n",
                "    'learning_rate': 0.1,\n",
                "    'max_depth': 4,\n",
                "    'criterion': 'squared_error'\n",
                "}\n",
                "\n",
                "gb, score = gbClassifier(X_norm, y_encoded, **hyperparams)\n",
                "# gb, score = None,None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [],
            "source": [
                "### SKIP\n",
                "# This cell won't be marked. Use it to try out your code."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Random Forest**\n",
                "\n",
                "Fill in the function to perform boosting with  [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
                "\n",
                "Fill in the function to perform Random Forest. Test your function with Random Forest 200 estimators, log loss entropy, max depth of 4 and 12 max leaf nodes.\n",
                "\n",
                "The format of your output should be:\n",
                "\n",
                "Mean cross-validation score: x.xx\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Mean cross-validation score: 0.77\n"
                }
            ],
            "source": [
                "### TEST FUNCTION: test_rf\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "#Random Forest\n",
                "def rfClassifier(X, y, **hyperparams):\n",
                "    \"\"\"Fill this function to run the Random Forest classifier as described above\"\"\"\n",
                "    # Initialize Random Forest classifier\n",
                "    fixed_hyperparams = {'random_state': 0}\n",
                "    fixed_hyperparams.update(hyperparams)\n",
                "    rf = RandomForestClassifier(**fixed_hyperparams)\n",
                "    cvKFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
                "    cv_scores = cross_val_score(rf, X, y, cv=cvKFold)\n",
                "    mean_cv_score = np.mean(cv_scores)\n",
                "    \n",
                "    print(f\"Mean cross-validation score: {mean_cv_score:.2f}\")\n",
                "    return rf, mean_cv_score\n",
                "\n",
                "hyperparams = {\n",
                "    'n_estimators': 200,\n",
                "    'criterion': 'entropy',\n",
                "    'max_depth': 4,\n",
                "    'max_leaf_nodes': 12,\n",
                "    'random_state': 0\n",
                "}\n",
                "\n",
                "rf, score = rfClassifier(X_norm, y_encoded, **hyperparams)\n",
                "# rf, score = None,None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "metadata": {},
            "outputs": [],
            "source": [
                "### SKIP\n",
                "# This cell won't be marked. Use it to try out your code."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parameter tuning **without** cross-validation"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Cross-validation is an excellent tool for determining the best generalisation performance and determining the best hyperparameters, but is not always appropriate for large datasets and/or large number of hyperparameters.\n",
                "\n",
                "For one classifier, Adaboost, we would like to find the best parameters using grid search without using cross-validation.\n",
                "\n",
                "The data should be **split** into a full training subset and a hold-out test subset using [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) as below. Make sure to use stratification and random_state=0. You should then split the full training set into a training set and validation set for manual grid search without cross-validation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_assert_splitting\n",
                "# Create this function to use on any subset of this data\n",
                "from sklearn.model_selection import train_test_split\n",
                "# TODO: uncomment this code to create the initial train test split\n",
                "def train_val_test_split(X,y):\n",
                "     \"\"\"Fill this function split the data into training, validation and test sets as described above\"\"\"\n",
                "     X_train_all, X_test, y_train_all, y_test = train_test_split(X, y,random_state=0, test_size = 0.25, stratify=y)\n",
                "     X_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all\n",
                "     , stratify=y_train_all, random_state=0, test_size = 0.1)\n",
                "    \n",
                "     return X_train_all, X_train, X_val, X_test, y_train_all, y_train, y_val, y_test\n",
                "X_train_all, X_train, X_val, X_test, y_train_all, y_train, y_val, y_test = train_val_test_split(X, y)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Training set size:  (518, 8) (518,)\nValidation set size:  (58, 8) (58,)\nTest set size:  (192, 8) (192,)\nAll Training set size:  (576, 8) (576,)\n"
                }
            ],
            "source": [
                "### SKIP\n",
                "# This cell won't be marked. Use it to try out your code.\n",
                "print(\"Training set size: \", X_train.shape, y_train.shape)\n",
                "print(\"Validation set size: \", X_val.shape, y_val.shape)\n",
                "print(\"Test set size: \", X_test.shape, y_test.shape)\n",
                "print(\"All Training set size: \", X_train_all.shape, y_train_all.shape)\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Adaboost**\n",
                "\n",
                "Perform a grid search for AdaBoost using Linear SVM as base classifier. You should select  and handle appropriate hyperparameters for both methods, and return the best set of hyperparameters found.\n",
                "\n",
                "In the following cell, you should define a grid with at least three parameters for the AdaBoost and/or the Linear SVM.\n",
                "\n",
                "Use the variable names provided in the scaffold. Try different ranges of hyperparameters to improve your classifier's performance."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Output format:\n",
                "\n",
                "Best hyperparameter combination: {'param1': value, 'param2': value, ...}\n",
                "\n",
                "Best model's test set score: x.xx"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "1 Best hyperparameter combination: {'algorithm': ['SAMME'], 'dual': ['auto']}\n1 Best model's test set score: 0.76\n"
                }
            ],
            "source": [
                "### TEST FUNCTION: test_parameter_tuning_no_cv\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.ensemble import AdaBoostClassifier\n",
                "from sklearn.svm import LinearSVC\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "def adaBoostGrid(X, y, **hyperparams):\n",
                "     \"\"\"Fill this function to run the Adaboost grid search on linear SVM without cross-validation as described above\"\"\"\n",
                "     X_train_all, X_train, X_val, X_test, y_train_all, y_train, y_val, y_test = train_val_test_split(X, y)\n",
                "     best_val_score = -1\n",
                "     best_params = {}\n",
                "     best_model = None\n",
                "     algorithm = hyperparams.get('algorithm', ['SAMME'])[0]\n",
                "\n",
                "     default_C = [1.0]\n",
                "     default_n_estimators = [50]\n",
                "     default_learning_rate = [1.0]\n",
                "     # if auto dual\n",
                "     dual = X_train.shape[0] \u003c X_train.shape[1] \n",
                "     print(dual)\n",
                "     if 'dual' not in hyperparams:\n",
                "          for C in hyperparams.get('estimator__C', default_C):\n",
                "               for n_estimators in hyperparams.get('n_estimators', default_n_estimators):\n",
                "                    for learning_rate in hyperparams.get('learning_rate', default_learning_rate):\n",
                "                    \n",
                "                         linear_svc = LinearSVC(C=C,dual = False, random_state=0)\n",
                "                         ada = AdaBoostClassifier(estimator=linear_svc, n_estimators=n_estimators, learning_rate=learning_rate, algorithm=algorithm, random_state=0)\n",
                "\n",
                "                         ada.fit(X_train, y_train)\n",
                "                         val_predict = ada.predict(X_val)\n",
                "                         val_score = accuracy_score(y_val, val_predict)\n",
                "                              \n",
                "                         if val_score \u003e best_val_score:\n",
                "                              best_val_score = val_score\n",
                "                              best_params = {'estimator__C': C, 'n_estimators': n_estimators, 'learning_rate': learning_rate, 'algorithm': algorithm}\n",
                "                              \n",
                "          linear_svc_best = LinearSVC(C=best_params['estimator__C'], random_state=0, dual = 'auto')\n",
                "          best_model = AdaBoostClassifier(estimator=linear_svc_best, n_estimators=best_params['n_estimators'], learning_rate=best_params['learning_rate'], \n",
                "          algorithm=best_params['algorithm'], random_state=0)\n",
                "     else:\n",
                "          best_model = AdaBoostClassifier(algorithm=algorithm, random_state=0)\n",
                "          best_params = hyperparams\n",
                "                              \n",
                "     \n",
                "     best_model.fit(X_train_all, y_train_all)\n",
                "     val_predict = best_model.predict(X_test)\n",
                "     test_score = accuracy_score(y_test, val_predict)\n",
                "     print(\"Best hyperparameter combination:\", best_params)\n",
                "     print(f\"Best model's test set score: {test_score:.2f}\")\n",
                "     \n",
                "\n",
                "     return best_model, best_params, best_val_score, test_score\n",
                "\n",
                "param_grid = {\n",
                "     'algorithm': ['SAMME'],\n",
                "    'estimator__C': [0.01, 0.1], \n",
                "    'learning_rate': [0.1, 1],\n",
                "    'n_estimators': [50, 100]\n",
                "    \n",
                "    \n",
                "}\n",
                "\n",
                "best_model, best_params, best_val_score, test_score = adaBoostGrid(X_norm, y_encoded, **param_grid)\n",
                "\n",
                "\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_adaboost_grid_no_cv\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "# best_model, best_params, best_val_score, test_score = adaBoostGrid(X_norm, y_encoded)\n",
                "# print(\"Best hyperparameter combination:\", best_params)\n",
                "# print(f\"Best model's test set score: {test_score:.2f}\")\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {},
            "outputs": [],
            "source": [
                "### SKIP\n",
                "# This cell won't be marked. Use it to try out your code."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parameter tuning **with** cross-validation"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Repeat the grid search above for Adaboost using Linear SVM as the base classifier with cross-validation using grid search with 10-fold stratified cross-validation with [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). \n",
                "\n",
                "The full training set from above should be used, and the trained classifier performance should be evaluated on the hold-out test set. \n",
                "\n",
                "You will need to pass cvKFold (the stratified folds) as an argument to GridSearchCV, not cv=10 as in the tutorials. This ensures that random_state=0 for the cross-validation. random_state=0 will still need to be set in the method constructors.\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_parameter_tuning_cv\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
                "from sklearn.ensemble import AdaBoostClassifier\n",
                "from sklearn.svm import LinearSVC\n",
                "from sklearn.metrics import accuracy_score\n",
                "def adaBoostGrid(X, y, **hyperparams):\n",
                "    \"\"\"Fill this function to run the Adaboost Grid search with cross-validation as described above\"\"\"\n",
                "    # print(hyperparams)\n",
                "    X_train_all, X_train, X_val, X_test, y_train_all, y_train, y_val, y_test = train_val_test_split(X, y)\n",
                "\n",
                "    du = hyperparams.get(\"dual\", True)\n",
                "    cvKFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
                "    linear_svc = LinearSVC(random_state=0, dual=False)\n",
                "    ada = AdaBoostClassifier(estimator=linear_svc, random_state=0)\n",
                "    \n",
                "    if 'dual' in hyperparams:\n",
                "        del hyperparams['dual']\n",
                "    # print(hyperparams)\n",
                "    grid_search = GridSearchCV(estimator=ada, param_grid=hyperparams, cv=cvKFold, scoring='accuracy', n_jobs=-1)\n",
                "    grid_search.fit(X_train_all, y_train_all)\n",
                "\n",
                "    # best_model = grid_search.best_estimator_\n",
                "    best_params = grid_search.best_params_\n",
                "    best_cv_score = grid_search.best_score_\n",
                "\n",
                "    # best_model.fit(X_train, y_train)\n",
                "    # y_pred = best_model.predict(X_test)\n",
                "    # test_score = accuracy_score(y_test, y_pred)\n",
                "    y_pred = grid_search.predict(X_test)\n",
                "    test_score = accuracy_score(y_test, y_pred)\n",
                "\n",
                "    print(\"2 Best hyperparameter combination:\", best_params)\n",
                "    print(f\"2 Best model's test set score: {test_score:.2f}\")\n",
                "    # best_model, best_params, best_cv_score, test_score = None,None, None, None\n",
                "    return grid_search, best_params, best_cv_score, test_score\n",
                "\n",
                "param_grid = {\n",
                "    'algorithm': ['SAMME'],\n",
                "    'estimator__C': [0.01, 0.1], \n",
                "    'n_estimators': [50, 100], \n",
                "    'learning_rate': [0.1, 1],\n",
                "    \n",
                "}\n",
                "\n",
                "best_model, best_params, best_cv_score, test_score = adaBoostGrid(X_norm, y_encoded, **param_grid)\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Helloworlsd\n"
                }
            ],
            "source": [
                "### TEST FUNCTION: test_adaboost_grid_cv\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "### SKIP\n",
                "# This cell won't be marked. Use it to try out your code."
            ]
        }
    ]
}
